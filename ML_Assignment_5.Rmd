---
title: "ML - Assignment 5"
output: html_notebook
---

### Initial environment and data setup.
```{r}
library(data.table)
library(magrittr)

data <- fread("../../data/medical-appointments-no-show/no-show-data.csv")

# some data cleaning
data[, c("PatientId", "AppointmentID", "Neighbourhood") := NULL]
setnames(data, 
         c("No-show", 
           "Age", 
           "Gender",
           "ScheduledDay", 
           "AppointmentDay",
           "Scholarship",
           "Hipertension",
           "Diabetes",
           "Alcoholism",
           "Handcap",
           "SMS_received"), 
         c("no_show", 
           "age", 
           "gender", 
           "scheduled_day", 
           "appointment_day",
           "scholarship",
           "hypertension",
           "diabetes",
           "alcoholism",
           "handicap",
           "sms_received"))

# for binary prediction, the target variable must be a factor
data[, no_show := factor(no_show, levels = c("Yes", "No"))]
data[, handicap := ifelse(handicap > 0, 1, 0)]

# create new variables
data[, gender := factor(gender)]
data[, scholarship := factor(scholarship)]
data[, hypertension := factor(hypertension)]
data[, alcoholism := factor(alcoholism)]
data[, handicap := factor(handicap)]

data[, scheduled_day := as.Date(scheduled_day)]
data[, appointment_day := as.Date(appointment_day)]
data[, days_since_scheduled := as.integer(appointment_day - scheduled_day)]

# clean up a little bit
data <- data[age %between% c(0, 95)]
data <- data[days_since_scheduled > -1]
data[, c("scheduled_day", "appointment_day", "sms_received") := NULL]

##Initializing h2o and loading the data.
library(h2o)
h2o.init()

data <- as.h2o(data)
```

## 1. Deep learning with h2o

### a. Create train / validation / test sets.

```{r}
data_split <- h2o.splitFrame(data, ratios = 0.5, seed = 123)
data_train <- data_split[[1]]
data_test <- data_split[[2]]

data_split <- h2o.splitFrame(data_test, ratios = 0.5, seed = 123)

data_val <- data_split[[1]]
data_test <- data_split[[2]]

##Defining Prdictor and target variables
y <- "no_show"
X <- setdiff(names(data_train), y)

```
### b. Train a benchmark model of your choice using h2o (such as random forest, gbm or glm) and evaluate it on the validation set.
```{r}
##Training bechmark model as glm
glm_model <- h2o.glm(
  X, y,
  training_frame = data_train,
  family = "binomial",
  alpha = 1, 
  lambda_search = TRUE,
  seed = 123,
  nfolds = 5
)
```
AUC of glm benchmark model is :

```{r}
print(h2o.auc(h2o.performance(glm_model, newdata = data_val)))
```
### c. Build deep learning models. Experiment with parameter settings.

#### Basic Deep Learning model.
```{r}
dl_model_basic <- h2o.deeplearning(
  X, y,
  training_frame = data_train,
  validation_frame = data_val,
  hidden = c(32, 8),
  seed = 123,
  nfolds = 5, 
  stopping_metric = "AUC",
  reproducible = TRUE
)

print(h2o.auc(h2o.performance(dl_model_basic, newdata = data_val)))
#0.7185214
```
#### Modifying Network topology: number of layers and nodes within layers

```{r}
dl_model_layers <- h2o.deeplearning(
  X, y,
  training_frame = data_train,
  validation_frame = data_val,
  hidden = c(5, 2),
  seed = 123,
  nfolds = 5, 
  stopping_metric = "AUC",
  reproducible = TRUE
)

print(h2o.auc(h2o.performance(dl_model_layers, newdata = data_val)))
#0.6847421
```
Reducing number of layers and number of nodes reduced the performance.      
Increasing number of nodes crashed the system.(SOme performance issues with my laptop).      
So I will continue to use 8 layesr with 32 nodes each configuration.      
#### activation function
```{r}
dl_model_activation <- h2o.deeplearning(
  X, y,
  training_frame = data_train,
  validation_frame = data_val,
  hidden = c(32, 8),
  seed = 123,
  nfolds = 5, 
  stopping_metric = "AUC",
  activation = "Tanh",
  reproducible = TRUE
)

print(h2o.auc(h2o.performance(dl_model_activation, newdata = data_val)))
#0.7188071

dl_model_activation2 <- h2o.deeplearning(
  X, y,
  training_frame = data_train,
  validation_frame = data_val,
  hidden = c(32, 8),
  seed = 123,
  nfolds = 5, 
  stopping_metric = "AUC",
  activation = "RectifierWithDropout",
  reproducible = TRUE
)

print(h2o.auc(h2o.performance(dl_model_activation2, newdata = data_val)))
#0.7154182
```
Tried different activation functions like Tanh, RectifierWithDropout.       
Default activation function i.e. Rectifier performed better.      

#### dropout (both hidden and input layers)

```{r}
dl_model_dropout <- h2o.deeplearning(
  X, y,
  training_frame = data_train,
  validation_frame = data_val,
  hidden = c(32, 8),
  seed = 123,
  nfolds = 5, 
  stopping_metric = "AUC",
  activation = "RectifierWithDropout",
  input_dropout_ratio = 0.2,
  hidden_dropout_ratios = c(0.6, 0.7),
  reproducible = TRUE
)

print(h2o.auc(h2o.performance(dl_model_dropout, newdata = data_val)))
#0.6689904
```
Tried other suggested droupout ratios in the documentation.
Default ratios performed better.

#### lasso, ridge regularization
```{r}
dl_model_regularization <- h2o.deeplearning(
  X, y,
  training_frame = data_train,
  validation_frame = data_val,
  hidden = c(32, 8),
  seed = 123,
  nfolds = 5, 
  stopping_metric = "AUC",
  l2 = 0.0001,
  reproducible = TRUE
)

print(h2o.auc(h2o.performance(dl_model_regularization, newdata = data_val)))
```
For Lasso regularization (l1) of different values, AUC was around 0.5     
For Ridge i.e. l2, performance slightly inproved for 0.0001.
SO I will continue to use RIdge regularization.

#### Early stopping (changing stopping rounds, tolerance) and number of epochs.
```{r}
dl_model_stop <- h2o.deeplearning(
  X, y,
  training_frame = data_train,
  validation_frame = data_val,
  hidden = c(32, 8),
  seed = 123,
  nfolds = 5, 
  stopping_metric = "AUC",
  l2 = 0.0001,
  stopping_rounds = 5,
  stopping_tolerance = 0.002,
  reproducible = TRUE
)

print(h2o.auc(h2o.performance(dl_model_stop, newdata = data_val)))

#Stopping round =10,  0.7210128
#Stopping round =2, 0.7184971

dl_model_epochs <- h2o.deeplearning(
  X, y,
  training_frame = data_train,
  validation_frame = data_val,
  hidden = c(32, 8),
  seed = 123,
  nfolds = 5, 
  stopping_metric = "AUC",
  l2 = 0.0001,
  stopping_rounds = 5,
  epochs = 20,
  reproducible = TRUE
)

print(h2o.auc(h2o.performance(dl_model_epochs, newdata = data_val)))
#0.7232043
```

Increasing Stopping rounds and epochs slight increased the performance.

### d, e. How does your best model compare to the benchmark model on the test set?
```{r}
print(h2o.auc(h2o.performance(dl_model_epochs, newdata = data_test)))
#0.7175229
```

## 2. Stacking with h2o

### a. Build at least 4 models of different families using cross validation, keeping cross validated predictions.

#### GLM Model.
```{r}
glm_model <- h2o.glm(
  X, y,
  training_frame = data_train,
  family = "binomial",
  alpha = 1, 
  lambda_search = TRUE,
  seed = 123,
  nfolds = 5,
  keep_cross_validation_predictions = TRUE
)
```

#### Random Forest Model.
```{r}
rf_model <- h2o.randomForest(
  X, y,
  training_frame = data_train,
  ntrees = 100,
  seed = 123,
  nfolds = 5, 
  keep_cross_validation_predictions = TRUE
)

```
#### GBM Model
```{r}

```

#### Deep Learning Model
```{r}
dl_model <- h2o.deeplearning(
  X, y,
  training_frame = data_train,
  hidden = c(32, 8),
  seed = 123,
  nfolds = 5,
  keep_cross_validation_predictions = TRUE
)
```


